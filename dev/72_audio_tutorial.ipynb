{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local.torch_basics import *\n",
    "from local.test import *\n",
    "from local.basics import *\n",
    "from local.data.all import *\n",
    "from local.vision.core import *\n",
    "from local.notebook.showdoc import show_doc\n",
    "from local.audio.core import *\n",
    "from local.audio.augment import *\n",
    "from local.vision.learner import *\n",
    "from local.vision.models.xresnet import *\n",
    "from local.metrics import *\n",
    "from local.callback.schedule import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Training a Voice Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/jupyter/.fastai/data/ST-AEDS-20180100_1-OS/ST-AEDS-20180100_1-OS')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URLs.SPEAKERS10 = 'ST-AEDS-20180100_1-OS'\n",
    "p10speakers = Config()['data_path'] / URLs.SPEAKERS10\n",
    "untar_data(URLs.SPEAKERS10, dest=p10speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/jupyter/.fastai/data/250_speakers/250-speakers')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Warning this dataset is ~8GB\n",
    "p250speakers = Config()['data_path'] / '250_speakers'\n",
    "untar_data(URLs.SPEAKERS250, fname=str(p250speakers)+'.tar', dest=p250speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = AudioGetter(\"\", recurse=True, folders=None)\n",
    "files_10  = x(p10speakers)\n",
    "files_250 = x(p250speakers)\n",
    "#original_aud = AudioItem.create(files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datablock and Basic End to End Training on 10 Speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AudioBlock(cls=AudioItem): return TransformBlock(type_tfms=cls.create, batch_tfms=IntToFloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auds = DataBlock(blocks=(AudioBlock, CategoryBlock),  \n",
    "                 get_items=get_audio_files, \n",
    "                 splitter=RandomSplitter(),\n",
    "                 get_y=lambda x: str(x).split('/')[-1][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [y for _,y in auds.datasource(p10speakers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify categories are being correctly assigned\n",
    "test_eq(min(cats).item(), 0)\n",
    "test_eq(max(cats).item(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crop 2s from the signal and turn it to a MelSpectrogram with no augmentation\n",
    "cfg_voice = AudioConfig.Voice()\n",
    "a2s = AudioToSpec.from_cfg(cfg_voice)\n",
    "crop_2000ms = CropSignal(2000)\n",
    "tfms = [crop_2000ms, a2s]\n",
    "dbunch = auds.databunch(p10speakers, item_tfms=tfms, bs=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-block alert-warning\"><strong>Broken:</strong><br>Show batch is broken as it appears to just be grabbing the data from the sg, and not the sg object itself, but calls the sg's show method which relies on nchannels, which is an object of AudioSpectrogram (part of sg settings but we overrode getattr to make it work like an attribute). This means the items cant show themselves for the batch, but training still works </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dbunch_cropspec.show_batch(max_n=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 128, 251])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbunch.one_batch()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit to Kevin Bird and Hiromi Suenaga for these two lines to adjust a CNN model to take 1 channel input\n",
    "def alter_learner(learn, channels=1):\n",
    "    learn.model[0][0].in_channels=channels\n",
    "    learn.model[0][0].weight = torch.nn.parameter.Parameter(learn.model[0][0].weight[:,1,:,:].unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dbunch, \n",
    "                xresnet18(),\n",
    "                torch.nn.CrossEntropyLoss(), \n",
    "                metrics=[accuracy])\n",
    "nchannels = dbunch.one_batch()[0].shape[1]\n",
    "alter_learner(learn, nchannels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#5) [0,1.9382734298706055,1.9040651321411133,0.48828125,00:12]\n",
      "(#5) [1,0.8095316886901855,4.755290985107422,0.4518229067325592,00:11]\n",
      "(#5) [2,0.4113352596759796,1.9035459756851196,0.6393229365348816,00:11]\n",
      "(#5) [3,0.24505342543125153,1.2673859596252441,0.6497395634651184,00:11]\n",
      "(#5) [4,0.14540395140647888,1.8934131860733032,0.65234375,00:11]\n",
      "(#5) [5,0.10805315524339676,0.33648693561553955,0.91015625,00:11]\n",
      "(#5) [6,0.07249930500984192,0.09889302402734756,0.9674479365348816,00:11]\n",
      "(#5) [7,0.04562441259622574,0.012994271703064442,0.99609375,00:11]\n",
      "(#5) [8,0.032863594591617584,0.019431808963418007,0.9908854365348816,00:11]\n",
      "(#5) [9,0.02620488777756691,0.007657718379050493,0.9986979365348816,00:11]\n"
     ]
    }
   ],
   "source": [
    "#epochs are a bit longer due to the chosen melspectrogram settings\n",
    "learn.fit_one_cycle(10, lr_max=slice(1e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on 250 Speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44655"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files_250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.fastai/data/250_speakers/250-speakers/id09068/FgsdnwUeLbQ/00034.wav\n",
      "/home/jupyter/.fastai/data/250_speakers/250-speakers/id08931/VWkkyFseXq0/00093.wav\n",
      "/home/jupyter/.fastai/data/250_speakers/250-speakers/id08925/bHZIObj5KRc/00072.wav\n",
      "/home/jupyter/.fastai/data/250_speakers/250-speakers/id09193/oQ_wAYpETVo/00215.wav\n",
      "/home/jupyter/.fastai/data/250_speakers/250-speakers/id09215/LL5rVd3_klU/00023.wav\n",
      "/home/jupyter/.fastai/data/250_speakers/250-speakers/id09143/o0WYXM0AHSU/00053.wav\n",
      "/home/jupyter/.fastai/data/250_speakers/250-speakers/id08938/Tza5wEVSwDg/00018.wav\n",
      "/home/jupyter/.fastai/data/250_speakers/250-speakers/id09210/Krc8rw5hITI/00232.wav\n",
      "/home/jupyter/.fastai/data/250_speakers/250-speakers/id09269/1-ycYhStJ4U/00042.wav\n",
      "/home/jupyter/.fastai/data/250_speakers/250-speakers/id09226/dia1ew89HiQ/00053.wav\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(random.choice(files_250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lens = stats(files_250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_250speakers_label = lambda x: str(x).split('/')[-3][3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: /home/jupyter/.fastai/data/250_speakers/250-speakers/id08890/39y4SHOk_XU/00021.wav\n",
      "Label: 8890\n",
      "File: /home/jupyter/.fastai/data/250_speakers/250-speakers/id08902/V8iO80JAlIA/00144.wav\n",
      "Label: 8902\n",
      "File: /home/jupyter/.fastai/data/250_speakers/250-speakers/id09155/jC9uOAwymcA/00223.wav\n",
      "Label: 9155\n",
      "File: /home/jupyter/.fastai/data/250_speakers/250-speakers/id08933/5zyBRP4EaSI/00057.wav\n",
      "Label: 8933\n",
      "File: /home/jupyter/.fastai/data/250_speakers/250-speakers/id09112/oFhWPmQ1M3Q/00072.wav\n",
      "Label: 9112\n",
      "File: /home/jupyter/.fastai/data/250_speakers/250-speakers/id08885/ymKY1RwnHR0/00476.wav\n",
      "Label: 8885\n",
      "File: /home/jupyter/.fastai/data/250_speakers/250-speakers/id08993/0BUR_R4VOD8/00010.wav\n",
      "Label: 8993\n",
      "File: /home/jupyter/.fastai/data/250_speakers/250-speakers/id09012/cH0KIzU7l4s/00077.wav\n",
      "Label: 9012\n",
      "File: /home/jupyter/.fastai/data/250_speakers/250-speakers/id09031/40lzCrU9WC4/00027.wav\n",
      "Label: 9031\n",
      "File: /home/jupyter/.fastai/data/250_speakers/250-speakers/id09193/Ow_zRKUyITc/00166.wav\n",
      "Label: 9193\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    f = random.choice(files_250)\n",
    "    print(\"File:\",f )\n",
    "    print(\"Label:\", get_250speakers_label(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auds = DataBlock(blocks=(AudioBlock, CategoryBlock),  \n",
    "                 get_items=get_audio_files, \n",
    "                 splitter=RandomSplitter(),\n",
    "                 get_y=get_250speakers_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch250 = auds.databunch(p250speakers, item_tfms=tfms, bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastprogress import progress_bar as pb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='44655' class='' max='44655', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [44655/44655 04:22<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cats = [y for _,y in pb(auds.datasource(p250speakers))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify categories are being correctly assigned for 250 speakers\n",
    "test_eq(min(cats).item(), 0)\n",
    "test_eq(max(cats).item(), 249)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use torchaudio default MelSpectrogram to get a baseline\n",
    "a2s = AudioToSpec()\n",
    "crop_4000ms = CropSignal(4000)\n",
    "tfms = [crop_4000ms, a2s]\n",
    "dbunch = auds.databunch(p250speakers, item_tfms=tfms, bs=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dbunch, \n",
    "                xresnet18(),\n",
    "                torch.nn.CrossEntropyLoss(), \n",
    "                metrics=[accuracy])\n",
    "nchannels = dbunch.one_batch()[0].shape[1]\n",
    "alter_learner(learn, nchannels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, lr_max=slice(2e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(5, lr_max=slice(1e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize our AudioToSpec Function using a config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_cfg = AudioConfig.Voice()\n",
    "a2s = AudioToSpec.from_cfg(voice_cfg)\n",
    "tfms = [crop_4000ms, a2s]\n",
    "# tfms = Pipeline([CropSignal(4000),  a2s, MaskFreq(size=12), MaskTime(size=15), SGRoll()], as_item=True)\n",
    "dbunch = auds.databunch(p250speakers, item_tfms=tfms, bs=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dbunch, \n",
    "                xresnet18(),\n",
    "                torch.nn.CrossEntropyLoss(), \n",
    "                metrics=[accuracy])\n",
    "nchannels = dbunch.one_batch()[0].shape[1]\n",
    "alter_learner(learn, nchannels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better results even without fine tuning, but much slower. We need to move a2s to the GPU and \n",
    "# then add data augmentation!\n",
    "learn.fit_one_cycle(5, lr_max=slice(2e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an MFCC with Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only grab 1500ms of the clip, voice identity can be done with shorter sections and it will speed it up\n",
    "# this is really slow for mfcc, even for 45k files, need to figure out what's going on here\n",
    "a2mfcc = AudioToMFCC(n_mffc=20, melkwargs={\"n_fft\":2048, \"hop_length\":256, \"n_mels\":128})\n",
    "tfms = [CropSignal(1500), a2mfcc, Delta()]\n",
    "# tfms = Pipeline([CropSignal(4000),  a2s, MaskFreq(size=12), MaskTime(size=15), SGRoll()], as_item=True)\n",
    "dbunch = auds.databunch(p250speakers, item_tfms=tfms, bs=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dbunch, \n",
    "                xresnet18(),\n",
    "                torch.nn.CrossEntropyLoss(), \n",
    "                metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, lr_max=slice(2e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(7, lr_max=slice(3e-3, 4e-3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'><strong>From Here:</strong><br>\n",
    "    1. Get transforms on the GPU <br>\n",
    "    2. Once it's faster test signal and spectrogram augments for speed/efficacy<br>\n",
    "    3. Fine-tune and see how high we can push results on 250 speakers\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imperfect GPU Spectrogram Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaskFreqBatch(num_masks=1, size=20, start=None, val=None, **kwargs):\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    def _inner(spectro:AudioSpectrogram)->AudioSpectrogram:\n",
    "        '''Google SpecAugment time masking from https://arxiv.org/abs/1904.08779.'''\n",
    "        start_ = start\n",
    "        sg = spectro.clone()\n",
    "        channel_mean = sg.contiguous().view(sg.size(0), -1).mean(-1)[:,None,None]\n",
    "        mask_val = channel_mean if val is None else val\n",
    "        c, y, x = sg.shape[-3:]\n",
    "        for _ in range(num_masks):\n",
    "            #print(\"Mask Val\", mask_val)\n",
    "            #Currently not worrying about getting the channel mean from batchwise implementation\n",
    "            mask = torch.ones(size, x, device=device).unsqueeze(0).unsqueeze(0) #* mask_val    \n",
    "            if start_ is None: start_= random.randint(0, y-size)\n",
    "            if not 0 <= start_ <= y-size:\n",
    "                raise ValueError(f\"Start value '{start_}' out of range for AudioSpectrogram of shape {sg.shape}\")\n",
    "            sg[...,start_:start_+size,:] = mask\n",
    "            start_ = None\n",
    "        return AudioSpectrogram.create(sg, settings=spectro.settings)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path('/home/jupyter/.fastai/data/ST-AEDS-20180100_1-OS')\n",
    "cropsig_2000ms = CropSignal(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGRollBatch(max_shift_pct=0.5, direction=0, **kwargs):\n",
    "    '''Shifts spectrogram along x-axis wrapping around to other side'''\n",
    "    if int(direction) not in [-1, 0, 1]: \n",
    "        raise ValueError(\"Direction must be -1(left) 0(bidirectional) or 1(right)\")\n",
    "    def _inner(spectro:AudioSpectrogram)->AudioSpectrogram:\n",
    "        nonlocal direction\n",
    "        direction = random.choice([-1, 1]) if direction == 0 else direction\n",
    "        sg = spectro.clone()\n",
    "        c, height, width = sg.shape[-3:]\n",
    "        roll_by = int(width*random.random()*max_shift_pct*direction)\n",
    "        sg = sg.roll(roll_by, dims=-1)\n",
    "        return AudioSpectrogram.create(sg, settings=spectro.settings)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tfms = Pipeline([cropsig_2000ms, a2s_baseline], as_item=True)\n",
    "dl_tfms = Pipeline([Cuda(), MaskFreqBatch(), SGRollBatch()], as_item=True)\n",
    "dbunch = auds.databunch(p, item_tfms=ds_tfms, batch_tfms=dl_tfms, bs=64)\n",
    "dbunch.show_batch(max_n=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch.one_batch()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dbunch,\n",
    "                xresnet18(),  \n",
    "                torch.nn.CrossEntropyLoss(), \n",
    "                metrics=[accuracy])\n",
    "alter_learner(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(10, lr_max=2e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd Attempt with correct mean channel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaskFreqBatchValue(num_masks=1, size=20, start=None, val=None, **kwargs):\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    def _inner(spectro:AudioSpectrogram)->AudioSpectrogram:\n",
    "        '''Google SpecAugment time masking from https://arxiv.org/abs/1904.08779.'''\n",
    "        start_ = start\n",
    "        sg = spectro.clone()\n",
    "        channel_mean = sg.contiguous().view(sg.size(0), -1).mean(-1)[:,None,None,None]\n",
    "        mask_val = channel_mean if val is None else val\n",
    "        b, c, y, x = sg.shape\n",
    "        for _ in range(num_masks):\n",
    "            ones_mask = torch.ones(size=(1, c, size, x), device=device)\n",
    "            mask = ones_mask * mask_val\n",
    "            if start_ is None: start_= random.randint(0, y-size)\n",
    "            if not 0 <= start_ <= y-size:\n",
    "                raise ValueError(f\"Start value '{start_}' out of range for AudioSpectrogram of shape {sg.shape}\")\n",
    "            sg[:,:,start_:start_+size,:] = mask\n",
    "            start_ = None\n",
    "        return AudioSpectrogram.create(sg, settings=spectro.settings)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.ones(size=(64,1,20,128)) * 12).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tfms = Pipeline([cropsig_2000ms, a2s_baseline], as_item=True)\n",
    "dl_tfms = Pipeline([Cuda(), MaskFreqBatchValue()], as_item=True)\n",
    "dbunch = auds.databunch(p, item_tfms=ds_tfms, batch_tfms=dl_tfms, bs=64)\n",
    "dbunch.show_batch(max_n=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3rd Attempt with correct random placement of mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MaskFreqBatchPlace(num_masks=1, size=20, start=None, val=None, **kwargs):\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    def _inner(spectro:AudioSpectrogram)->AudioSpectrogram:\n",
    "        '''Google SpecAugment time masking from https://arxiv.org/abs/1904.08779.'''\n",
    "        start_ = start\n",
    "        sg = spectro.clone()\n",
    "        channel_mean = sg.contiguous().view(sg.size(0), -1).mean(-1)[:,None,None,None]\n",
    "        mask_val = channel_mean if val is None else val\n",
    "        b, c, y, x = sg.shape\n",
    "        for _ in range(num_masks):\n",
    "            ones_mask = torch.ones(size=(1, c, size, x), device=device)\n",
    "            mask = ones_mask * mask_val\n",
    "            if start_ is None: start_= random.randint(0, y-size)\n",
    "            if not 0 <= start_ <= y-size:\n",
    "                raise ValueError(f\"Start value '{start_}' out of range for AudioSpectrogram of shape {sg.shape}\")\n",
    "            start_test = np.arange(0,,1)\n",
    "            #print(\"Index: shape\", start_test.shape)\n",
    "            sg[:,:,start_:start_+size,:] = mask\n",
    "            start_ = None\n",
    "        return AudioSpectrogram.create(sg, settings=spectro.settings)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tfms = Pipeline([cropsig_2000ms, a2s_baseline], as_item=True)\n",
    "dl_tfms = Pipeline([Cuda(), MaskFreqBatchPlace()], as_item=True)\n",
    "dbunch = auds.databunch(p, item_tfms=ds_tfms, batch_tfms=dl_tfms, bs=4)\n",
    "dbunch.show_batch(max_n=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.ones(size=(64,1,20,128)) * 12).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_batch = torch.stack([CropTime(2000)(a2s(AudioItem.create(files[i]))) for i in range(64)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.array([2,4,6,8], dtype=np.intp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index + 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_batch[:,:,index:index+20,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_batch[:,:,torch.arange(0, 7).long(),:].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
